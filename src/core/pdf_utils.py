"""
PDF utility functions for Llestrade (formerly Forensic Psych Report Drafter).
Handles PDF file operations like splitting and merging.
"""

import json
import os
import shutil
import time
import re
from pathlib import Path

import fitz  # PyMuPDF
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeResult, DocumentContentFormat

# Azure Document Intelligence imports
from azure.core.credentials import AzureKeyCredential


def get_pdf_page_count(pdf_path):
    """
    Get the number of pages in a PDF file.

    Args:
        pdf_path: Path to the PDF file

    Returns:
        int: Number of pages in the PDF
    """
    doc = fitz.open(pdf_path)
    page_count = len(doc)
    doc.close()
    return page_count


def split_large_pdf(pdf_path, output_dir, max_pages=1750, overlap=10):
    """
    Split a large PDF into smaller segments with overlap.

    Args:
        pdf_path: Path to the PDF file to split
        output_dir: Directory to save the split PDF files
        max_pages: Maximum number of pages per segment
        overlap: Number of pages to overlap between segments

    Returns:
        list: Paths to the split PDF files
    """
    # Get filename without extension and the extension
    pdf_filename = os.path.basename(pdf_path)
    filename_base, ext = os.path.splitext(pdf_filename)

    # Open the PDF with PyMuPDF
    doc = fitz.open(pdf_path)
    total_pages = len(doc)

    # If the PDF is not large enough to split, return the original path
    if total_pages <= max_pages:
        doc.close()
        return [pdf_path]

    # Calculate how many segments we'll need
    segment_count = (total_pages - 1) // max_pages + 1
    output_files = []

    # Split the PDF into segments
    for i in range(segment_count):
        start_page = max(0, i * max_pages - overlap if i > 0 else 0)
        end_page = min(total_pages, (i + 1) * max_pages)

        # Create a new document for this segment
        new_doc = fitz.open()
        
        # Add the relevant pages to the new document
        new_doc.insert_pdf(doc, from_page=start_page, to_page=end_page-1)

        # Determine output filename for this segment
        output_filename = (
            f"{filename_base}_part{i+1:03d}_{start_page+1:05d}-{end_page:05d}{ext}"
        )
        output_path = os.path.join(output_dir, output_filename)

        # Save the new document
        new_doc.save(output_path)
        new_doc.close()

        output_files.append(output_path)

    # Close the original document
    doc.close()
    return output_files


def prepare_pdf_files(pdf_files, output_dir, max_pages=1750, overlap=10):
    """
    Process PDF files, splitting large ones and organizing them.

    Args:
        pdf_files: List of paths to PDF files
        output_dir: Directory to save processed PDF files
        max_pages: Maximum number of pages per segment
        overlap: Number of pages to overlap between segments

    Returns:
        tuple: (processed_files, temp_dir)
            processed_files: List of paths to processed PDF files
            temp_dir: Path to the temporary directory containing split files
    """
    # Create a temporary directory inside the output directory
    temp_dir = os.path.join(output_dir, "temp_pdf_processing")
    os.makedirs(temp_dir, exist_ok=True)

    processed_files = []

    for pdf_path in pdf_files:
        # Check the page count
        page_count = get_pdf_page_count(pdf_path)

        if page_count > max_pages:
            # Split the large PDF
            split_files = split_large_pdf(pdf_path, temp_dir, max_pages, overlap)
            processed_files.extend(split_files)
        else:
            # Just add the original file to the processed list
            processed_files.append(pdf_path)

    return processed_files, temp_dir


def cleanup_temp_files(temp_dir):
    """
    Remove the temporary directory and its contents.

    Args:
        temp_dir: Path to the temporary directory
    """
    shutil.rmtree(temp_dir, ignore_errors=True)


def extract_text_from_pdf(pdf_path):
    """
    Extract text from a PDF file using PyMuPDF.

    Args:
        pdf_path: Path to the PDF file to extract text from

    Returns:
        dict: Dictionary with success status and extracted text or error
    """
    try:
        doc = fitz.open(pdf_path)
        text = []
        
        for page_num, page in enumerate(doc):
            page_text = page.get_text()
            text.append(f"--- Page {page_num + 1} ---\n{page_text}")
        
        doc.close()
        
        return {
            "success": True,
            "text": "\n\n".join(text)
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }


def extract_text_from_pdf_with_azure(pdf_path, endpoint=None, key=None):
    """
    Extract text from a PDF file using Azure Document Intelligence.

    Args:
        pdf_path: Path to the PDF file to extract text from
        endpoint: Azure Document Intelligence API endpoint
        key: Azure Document Intelligence API key

    Returns:
        dict: Dictionary with success status and extracted text or error
    """
    try:
        # Use a temporary output directory
        temp_dir = os.path.join(os.path.dirname(pdf_path), "temp_azure_extraction")
        os.makedirs(temp_dir, exist_ok=True)
        
        # Process with Azure
        try:
            json_path, markdown_path = process_pdf_with_azure(
                pdf_path, temp_dir, endpoint=endpoint, key=key
            )
            
            # Read the markdown content
            with open(markdown_path, "r", encoding="utf-8") as f:
                markdown_content = f.read()
            
            # Clean up temporary files
            shutil.rmtree(temp_dir, ignore_errors=True)
            
            return {
                "success": True,
                "text": markdown_content
            }
        except Exception as e:
            # If Azure fails, fall back to local processing
            shutil.rmtree(temp_dir, ignore_errors=True)
            return extract_text_from_pdf(pdf_path)
            
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }


def process_pdf_with_azure(
    pdf_path, output_dir, json_dir=None, markdown_dir=None, endpoint=None, key=None
):
    """
    Process a PDF file using Azure Document Intelligence.

    Args:
        pdf_path: Path to the PDF file to process
        output_dir: Base output directory
        json_dir: Directory for JSON output. If None, JSON will NOT be written.
        markdown_dir: Directory for markdown output (if None, will be created in output_dir)
        endpoint: Azure Document Intelligence endpoint
        key: Azure Document Intelligence API key

    Returns:
        tuple: (json_path, markdown_path) - Paths to the created files

    Raises:
        ValueError: If the Azure endpoint or key is not provided
        Exception: If there's an error processing the file
    """
    # Check for Azure credentials
    if not endpoint or not key:
        # Look for environment variables
        endpoint = os.getenv("AZURE_ENDPOINT")
        key = os.getenv("AZURE_KEY")

        if not endpoint or not key:
            raise ValueError(
                "Azure endpoint and key must be provided or set as environment variables"
            )

    # Validate the endpoint format
    if not endpoint.startswith("https://"):
        raise ValueError(
            f"Invalid Azure endpoint format: {endpoint}. It should be a complete URL starting with https://"
        )

    # Create output directories if needed
    if not markdown_dir:
        markdown_dir = os.path.join(output_dir, "markdown")
    
    if json_dir is not None:
        os.makedirs(json_dir, exist_ok=True)
    os.makedirs(markdown_dir, exist_ok=True)

    # Get the file name for the output files
    file_name = os.path.basename(pdf_path)
    base_name = os.path.splitext(file_name)[0]

    # Define output file paths
    json_path = os.path.join(json_dir, f"{base_name}.json") if json_dir is not None else None
    markdown_path = os.path.join(markdown_dir, f"{base_name}.md")

    # Skip if both files already exist
    if json_path is not None:
        if os.path.exists(json_path) and os.path.exists(markdown_path):
            print(f"Skipping {file_name} - already converted (found {base_name}.json and {base_name}.md)")
            return json_path, markdown_path
    else:
        if os.path.exists(markdown_path):
            print(f"Skipping {file_name} - already converted (found {base_name}.md)")
            return None, markdown_path

    # Initialize the Document Intelligence client with retry mechanism
    max_retries = 3
    retry_delay = 2  # seconds
    
    for attempt in range(1, max_retries + 1):
        try:
            print(f"Attempt {attempt}/{max_retries} to connect to Azure Document Intelligence")
            document_intelligence_client = DocumentIntelligenceClient(
                endpoint=endpoint, credential=AzureKeyCredential(key)
            )
            break  # Successfully created the client
        except Exception as e:
            if attempt == max_retries:
                raise Exception(
                    f"Failed to initialize Azure Document Intelligence client after {max_retries} attempts: {str(e)}"
                )
            print(f"Connection attempt {attempt} failed: {str(e)}. Retrying in {retry_delay} seconds...")
            time.sleep(retry_delay)
            retry_delay *= 2  # Exponential backoff

    # Check if the file exists and is readable
    if not os.path.exists(pdf_path):
        raise Exception(f"PDF file not found: {pdf_path}")

    # Check the file size (Azure has different limits based on tier)
    file_size_mb = os.path.getsize(pdf_path) / (1024 * 1024)

    # We'll assume S0 tier by default, but provide helpful error messages
    # S0 (paid) tier: 500 MB
    # F0 (free) tier: 4 MB
    MAX_SIZE_MB = 500

    if file_size_mb > MAX_SIZE_MB:
        raise Exception(
            f"PDF file is too large for Azure Document Intelligence: {file_size_mb:.1f}MB "
            f"(S0 tier max: 500MB, F0 tier max: 4MB). "
            f"Consider splitting the file or using a different processing method."
        )
    elif file_size_mb > 4:
        # This might still fail if using F0 tier
        print(
            f"Warning: File size ({file_size_mb:.1f}MB) exceeds F0 tier limit (4MB). "
            f"Processing will only succeed if using S0 tier."
        )

    # Check page count (Azure has a 1000 page limit)
    try:
        page_count = get_pdf_page_count(pdf_path)
        MAX_PAGES = 1000

        if page_count > 2:
            # F0 tier only processes first 2 pages
            print(
                f"Warning: PDF has {page_count} pages. Note that F0 tier will only process the first 2 pages."
            )
    except Exception as e:
        # If we can't get page count, we'll still try to process
        print(
            f"Warning: Couldn't determine page count: {str(e)}. Continuing with processing."
        )

    # If page count exceeds Azure's 1000 page limit, process in ranges with overlap and combine
    if 'page_count' in locals() and page_count is not None and page_count > 1000:
        combined = _azure_markdown_chunked(
            client=document_intelligence_client,
            pdf_path=pdf_path,
            total_pages=page_count,
            max_pages=1000,
            overlap=5,
        )
        # Save combined markdown
        with open(markdown_path, "w", encoding="utf-8") as f:
            f.write(f"# {file_name}\n\n")
            f.write(combined)
        print(f"Saved chunked Markdown results to {markdown_path}")
        # JSON not produced for chunked path; caller does not require it
        return None, markdown_path

    # Process the file with Azure Document Intelligence (single pass)
    try:
        # Open the file
        with open(pdf_path, "rb") as file:
            # Add retry mechanism for processing operations
            max_proc_retries = 3
            proc_retry_delay = 3  # seconds
            
            # Process for markdown with retries
            markdown_result = None
            for attempt in range(1, max_proc_retries + 1):
                try:
                    print(f"Markdown processing attempt {attempt}/{max_proc_retries}")
                    # Process for markdown
                    print(
                        f"Starting Azure Document Intelligence processing for {os.path.basename(pdf_path)} in Markdown format"
                    )
                    markdown_poller = document_intelligence_client.begin_analyze_document(
                        "prebuilt-layout",
                        file,
                        output_content_format=DocumentContentFormat.MARKDOWN,
                    )
                    
                    # Get the markdown results with timeout handling
                    print(f"Waiting for Azure Document Intelligence Markdown results...")
                    markdown_result = markdown_poller.result(timeout=1800)  # 30 minute timeout
                    print(f"Received Azure Document Intelligence Markdown results")
                    break  # Success, exit the retry loop
                except Exception as e:
                    if attempt == max_proc_retries:
                        raise Exception(f"Failed to process PDF for Markdown after {max_proc_retries} attempts: {str(e)}")
                    print(f"Markdown processing attempt {attempt} failed: {str(e)}. Retrying in {proc_retry_delay} seconds...")
                    time.sleep(proc_retry_delay)
                    proc_retry_delay *= 1.5  # Increase delay for next attempt
                    file.seek(0)  # Reset file pointer for next attempt
            
            # Reset file pointer for JSON processing
            file.seek(0)
            
        json_result = None
        if json_path is not None:
            proc_retry_delay = 3
            for attempt in range(1, max_proc_retries + 1):
                try:
                    print(f"JSON processing attempt {attempt}/{max_proc_retries}")
                    print(
                        f"Starting Azure Document Intelligence processing for {os.path.basename(pdf_path)} in JSON format"
                    )
                    json_poller = document_intelligence_client.begin_analyze_document(
                        "prebuilt-layout", file
                    )
                    print("Waiting for Azure Document Intelligence JSON results...")
                    json_result = json_poller.result(timeout=1800)
                    print("Received Azure Document Intelligence JSON results")
                    break
                except Exception as e:
                    if attempt == max_proc_retries:
                        raise Exception(
                            f"Failed to process PDF for JSON after {max_proc_retries} attempts: {str(e)}"
                        )
                    print(
                        f"JSON processing attempt {attempt} failed: {str(e)}. Retrying in {proc_retry_delay} seconds..."
                    )
                    time.sleep(proc_retry_delay)
                    proc_retry_delay *= 1.5
                    file.seek(0)

        if not markdown_result:
            raise Exception("Failed to get Markdown results from Azure")

        # Save markdown content
        try:
            with open(markdown_path, "w", encoding="utf-8") as f:
                f.write(f"# {file_name}\n\n")
                f.write(markdown_result.content)
            print(f"Saved Markdown results to {markdown_path}")
        except Exception as e:
            raise Exception(f"Error saving Markdown results: {str(e)}")

        # Save JSON content only if requested
        if json_path is not None and json_result is not None:
            try:
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump(json_result.as_dict(), f, indent=4)
                print(f"Saved JSON results to {json_path}")
            except Exception as e:
                raise Exception(f"Error saving JSON results: {str(e)}")

        return json_path, markdown_path

    except Exception as e:
        raise Exception(f"Error processing {pdf_path} with Azure: {str(e)}")


def _azure_markdown_chunked(*, client: DocumentIntelligenceClient, pdf_path: str, total_pages: int, max_pages: int, overlap: int) -> str:
    """Return combined Markdown by analyzing the PDF in page ranges with overlap.

    - Uses the `pages` parameter if supported; otherwise falls back to pre-splitting the PDF.
    - Ensures a PageBreak between pages and at range boundaries.
    - Deduplicates the first `overlap` pages of each chunk beyond the first.
    """
    ranges = []
    start = 1
    while start <= total_pages:
        end = min(total_pages, start + max_pages - 1)
        # extend overlap except for last chunk
        if end < total_pages:
            end = min(total_pages, end + overlap)
        ranges.append((start, end))
        if end == total_pages:
            break
        start = end - overlap + 1

    def _split_pages(markdown: str) -> list[str]:
        # Split by explicit Azure page breaks, preserving content segments per page
        pat = re.compile(r"^\s*<!--\s*PageBreak\s*-->\s*$", re.IGNORECASE | re.MULTILINE)
        # Normalize endings
        md = markdown.replace("\r\n", "\n")
        parts = pat.split(md)
        return [p.strip("\n") for p in parts]

    combined_segments: list[str] = []
    first = True
    for (rs, re_) in ranges:
        use_pages_param = True
        content = None
        try:
            with open(pdf_path, "rb") as fh:
                poller = client.begin_analyze_document(
                    "prebuilt-layout",
                    fh,
                    output_content_format=DocumentContentFormat.MARKDOWN,
                    pages=f"{rs}-{re_}",
                )
                result = poller.result(timeout=1800)
                content = result.content
        except TypeError:
            use_pages_param = False
        except Exception:
            # Some SDKs may not accept pages for file streams
            use_pages_param = False

        if not use_pages_param:
            # Fallback: render only the page range via temporary sub-PDF
            import fitz
            doc = fitz.open(pdf_path)
            sub = fitz.open()
            sub.insert_pdf(doc, from_page=rs - 1, to_page=re_ - 1)
            tmp_path = os.path.join(os.path.dirname(pdf_path), f".__tmp_azure_{rs}_{re_}.pdf")
            try:
                sub.save(tmp_path)
                sub.close()
                doc.close()
                with open(tmp_path, "rb") as fh:
                    poller = client.begin_analyze_document(
                        "prebuilt-layout",
                        fh,
                        output_content_format=DocumentContentFormat.MARKDOWN,
                    )
                    result = poller.result(timeout=1800)
                    content = result.content
            finally:
                try:
                    os.remove(tmp_path)
                except Exception:
                    pass

        if not content:
            continue

        segs = _split_pages(content)
        # Drop first `overlap` pages for subsequent chunks
        if not first:
            segs = segs[overlap:] if len(segs) > overlap else []

        if not segs:
            first = False
            continue

        # Append with explicit PageBreaks between pages and at chunk boundaries
        if combined_segments:
            combined_segments.append("<!-- PageBreak -->")
        interleaved = []
        for i, s in enumerate(segs):
            if i > 0:
                interleaved.append("<!-- PageBreak -->")
            interleaved.append(s)
        combined_segments.extend(interleaved)
        first = False

    combined = "\n\n".join(combined_segments).strip() + "\n"
    return combined


def process_pdfs_with_azure(pdf_files, output_dir, endpoint=None, key=None):
    """
    Process multiple PDF files using Azure Document Intelligence.

    Args:
        pdf_files: List of paths to PDF files
        output_dir: Base output directory
        endpoint: Azure Document Intelligence endpoint
        key: Azure Document Intelligence API key

    Returns:
        dict: Dictionary with information about processed files
    """
    # Create output directories
    json_dir = os.path.join(output_dir, "json")
    markdown_dir = os.path.join(output_dir, "markdown")

    os.makedirs(json_dir, exist_ok=True)
    os.makedirs(markdown_dir, exist_ok=True)

    # Track processing results
    results = {
        "total": len(pdf_files),
        "processed": 0,
        "skipped": 0,
        "failed": 0,
        "files": [],
    }

    # Process each file
    for pdf_path in pdf_files:
        try:
            # Check if files already exist
            file_name = os.path.basename(pdf_path)
            base_name = os.path.splitext(file_name)[0]
            json_path = os.path.join(json_dir, f"{base_name}.json")
            markdown_path = os.path.join(markdown_dir, f"{base_name}.md")

            if os.path.exists(json_path) and os.path.exists(markdown_path):
                # Skip if both files already exist
                results["skipped"] += 1
                results["files"].append(
                    {
                        "pdf": pdf_path,
                        "status": "skipped",
                        "json": json_path,
                        "markdown": markdown_path,
                    }
                )
                continue

            # Process the file
            json_path, markdown_path = process_pdf_with_azure(
                pdf_path, output_dir, json_dir, markdown_dir, endpoint, key
            )

            results["processed"] += 1
            results["files"].append(
                {
                    "pdf": pdf_path,
                    "status": "processed",
                    "json": json_path,
                    "markdown": markdown_path,
                }
            )

        except Exception as e:
            results["failed"] += 1
            results["files"].append(
                {"pdf": pdf_path, "status": "failed", "error": str(e)}
            )

    return results


def test_azure_connection(endpoint=None, key=None):
    """
    Test the connection to Azure Document Intelligence without processing a document.

    Args:
        endpoint: Azure Document Intelligence endpoint
        key: Azure Document Intelligence API key

    Returns:
        dict: Information about the test result

    Raises:
        Exception: If there's an error connecting to Azure
    """
    # Check for Azure credentials
    if not endpoint or not key:
        # Look for environment variables
        endpoint = os.getenv("AZURE_ENDPOINT")
        key = os.getenv("AZURE_KEY")

        if not endpoint or not key:
            return {
                "success": False,
                "error": "Azure endpoint and key must be provided or set as environment variables",
            }

    # Validate the endpoint format
    if not endpoint.startswith("https://"):
        return {
            "success": False,
            "error": f"Invalid Azure endpoint format: {endpoint}. It should be a complete URL starting with https://",
        }

    # Try to initialize the Document Intelligence client
    try:
        from azure.ai.documentintelligence import DocumentIntelligenceClient
        from azure.core.credentials import AzureKeyCredential

        # Initialize the client
        document_intelligence_client = DocumentIntelligenceClient(
            endpoint=endpoint, credential=AzureKeyCredential(key)
        )

        # Try a simple operation to verify connection
        # Instead of list_operations (which doesn't exist), we'll use a simple API call
        # that just validates the credentials without processing a document

        # Get the SDK version to display in the successful connection message
        import azure.ai.documentintelligence

        sdk_version = azure.ai.documentintelligence.__version__

        # If we got here, the connection is working - credentials are valid
        # and the endpoint exists
        return {
            "success": True,
            "message": "Successfully connected to Azure Document Intelligence",
            "endpoint": endpoint,
            "sdk_version": sdk_version,
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"Error connecting to Azure Document Intelligence: {str(e)}",
        }
